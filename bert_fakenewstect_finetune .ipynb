{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步：设置python版本为3.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture captured_output\n",
    "!/home/ma-user/anaconda3/bin/conda create -n python-3.9.0 python=3.9.0 -y --override-channels --channel https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main\n",
    "!/home/ma-user/anaconda3/envs/python-3.9.0/bin/pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "data = {\n",
    "   \"display_name\": \"python-3.9.0\",\n",
    "   \"env\": {\n",
    "      \"PATH\": \"/home/ma-user/anaconda3/envs/python-3.9.0/bin:/home/ma-user/anaconda3/envs/python-3.7.10/bin:/modelarts/authoring/notebook-conda/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/ma-user/modelarts/ma-cli/bin:/home/ma-user/modelarts/ma-cli/bin\"\n",
    "   },\n",
    "   \"language\": \"python\",\n",
    "   \"argv\": [\n",
    "      \"/home/ma-user/anaconda3/envs/python-3.9.0/bin/python\",\n",
    "      \"-m\",\n",
    "      \"ipykernel\",\n",
    "      \"-f\",\n",
    "      \"{connection_file}\"\n",
    "   ]\n",
    "}\n",
    "\n",
    "if not os.path.exists(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\"):\n",
    "    os.mkdir(\"/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/\")\n",
    "\n",
    "with open('/home/ma-user/anaconda3/share/jupyter/kernels/python-3.9.0/kernel.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注：以上代码运行完成后，需要重新设置kernel为python-3.9.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二步：安装MindSpore框架和MindNLP套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting mindspore==2.2.13\n",
      "  Using cached https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.2.13/MindSpore/unified/x86_64/mindspore-2.2.13-cp39-cp39-linux_x86_64.whl (756.2 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (5.27.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (2.4.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (10.4.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (24.1)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (6.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore==2.2.13) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore==2.2.13) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore==2.2.13) (0.43.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.2.13/MindSpore/unified/x86_64/mindspore-2.2.13-cp39-cp39-linux_x86_64.whl --trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Processing ./mindnlp-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: mindspore in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (2.2.13)\n",
      "Requirement already satisfied: tqdm in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (4.66.4)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (2.32.3)\n",
      "Requirement already satisfied: datasets in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (2.20.0)\n",
      "Requirement already satisfied: tokenizers in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.19.1)\n",
      "Requirement already satisfied: sentencepiece in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.2.0)\n",
      "Requirement already satisfied: regex in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (2024.5.15)\n",
      "Requirement already satisfied: easydict in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (1.13)\n",
      "Requirement already satisfied: safetensors in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.4.3)\n",
      "Requirement already satisfied: ml-dtypes in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindnlp==0.2.0) (0.4.0)\n",
      "Requirement already satisfied: filelock in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets->mindnlp==0.2.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (0.23.4)\n",
      "Requirement already satisfied: packaging in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from datasets->mindnlp==0.2.0) (6.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from requests->mindnlp==0.2.0) (2024.7.4)\n",
      "Requirement already satisfied: protobuf>=3.13.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (5.27.2)\n",
      "Requirement already satisfied: asttokens>=2.0.4 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (2.4.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (10.4.0)\n",
      "Requirement already satisfied: scipy>=1.5.4 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (1.13.1)\n",
      "Requirement already satisfied: psutil>=5.6.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (6.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from mindspore->mindnlp==0.2.0) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from asttokens>=2.0.4->mindspore->mindnlp==0.2.0) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from astunparse>=1.6.3->mindspore->mindnlp==0.2.0) (0.43.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from aiohttp->datasets->mindnlp==0.2.0) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets->mindnlp==0.2.0) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.2.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.2.0) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pandas->datasets->mindnlp==0.2.0) (2024.1)\n",
      "Installing collected packages: mindnlp\n",
      "  Attempting uninstall: mindnlp\n",
      "    Found existing installation: mindnlp 0.3.2\n",
      "    Uninstalling mindnlp-0.3.2:\n",
      "      Successfully uninstalled mindnlp-0.3.2\n",
      "Successfully installed mindnlp-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mindnlp-0.2.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 MindSpore 实现 BERT 虚假消息检测\n",
    "\n",
    "\n",
    "## 模型简介\n",
    "\n",
    "BERT全称是来自变换器的双向编码器表征量（Bidirectional Encoder Representations from Transformers），它是Google于2018年末开发并发布的一种新型语言模型。与BERT模型相似的预训练语言模型例如问答、命名实体识别、自然语言推理、文本分类等在许多自然语言处理任务中发挥着重要作用。\n",
    "\n",
    "BERT预训练之后，会保存它的Embedding table和12层Transformer权重（BERT-BASE）或24层Transformer权重（BERT-LARGE）。使用预训练好的BERT模型可以对下游任务进行Fine-tuning，比如：文本分类、相似度判断、阅读理解等。\n",
    "\n",
    "在当今信息爆炸的时代，虚假消息的传播给社会带来了巨大的负面影响，包括误导公众、损害个人或组织的声誉以及破坏社会稳定。因此，迅速准确地识别和过滤虚假消息变得至关重要。BERT语言模型作为一种强大的自然语言处理工具，具备理解语境和推断文本含义的能力，可以用于虚假消息的自动分类和检测。利用BERT语言模型进行虚假消息检测，可以有效应对当今信息泛滥和虚假信息传播的挑战，提高信息可信度，保护用户利益，提升用户体验，促进社会稳定，带来重要的商业和社会价值。\n",
    "\n",
    "我们使用LIAR数据集，把数据集变成“真/假”的二分类，然后通过BERT模型的训练，实现对一句话的真假性进行检测。\n",
    "\n",
    "下面就是实现BERT虚假消息检测的应用过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注：MindNLP whl包下载链接为：[MindNLP](https://repo.mindspore.cn/mindspore-lab/mindnlp/newest/any/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mindspore\n",
    "from mindspore.dataset import text, GeneratorDataset, transforms\n",
    "from mindspore import nn, context\n",
    "\n",
    "from mindnlp.engine import Trainer, Evaluator\n",
    "from mindnlp.engine.callbacks import CheckpointCallback, BestModelCallback\n",
    "from mindnlp.metrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentDataset:\n",
    "    \"\"\"Sentiment Dataset\"\"\"\n",
    "    def __init__(self, path):\n",
    "        # 初始化方法，加载数据集路径\n",
    "        self.path = path\n",
    "        self._labels, self._text_a = [], []\n",
    "        self._load()\n",
    "\n",
    "    def _load(self):\n",
    "        # 加载数据集文件并进行预处理\n",
    "        with open(self.path, \"r\", encoding=\"utf-8\") as f:\n",
    "            dataset = f.read()\n",
    "        lines = dataset.split(\"\\n\")\n",
    "        for line in lines[1:-1]:  # 跳过第一行（通常是表头）和最后一行（空行）\n",
    "            label, text_a = line.split(\"\\t\")[0:2]  # 读取标签和文本\n",
    "            self._labels.append(1 if label == \"TRUE\" else 0)  # 将标签转换为二进制形式\n",
    "            self._text_a.append(text_a)  # 保存文本内容\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 获取指定索引的数据（标签和文本）\n",
    "        return self._labels[index], self._text_a[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的长度\n",
    "        return len(self._labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集\n",
    "\n",
    "我们使用的是LIAR数据集。数据集包括来自POLITIFACT.COM API5的12.8K人类标记的简短语句，每个语句都由POLITIFACT.COM编辑器评估其真实性。设置false、half-true、true标签。和六种细粒度的真实性评级标签:pants-fire/FALSE/barely-true/half-true/mostly-true/TRUE。\n",
    "\n",
    "我们把数据集中除了真假标签和句子内容的列全部删除，并给他们加上标题\"label\"和\"text_a\"。\n",
    "接下来通过excel的查找替换功能，把每条数据的label列false/pants-fire/barely-true改为false，mostly-true/true改为true，把half-true删除，以此达到二分类的效果，如下示例。\n",
    "\n",
    "label--text_a\n",
    "\n",
    "TRUE--Heroin comes in the United States from the southern border.\n",
    "\n",
    "FALSE--Says every day of a special session costs taxpayers $40,000.\n",
    "\n",
    "TRUE--Our trade with Mexico is $720 million a day; thats our No. 1 trading partner.\n",
    "\n",
    "这部分主要包括数据集读取，数据格式转换，数据 Tokenize 处理和 pad 操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fndata' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "#下载数据集\n",
    "!git clone https://gitee.com/lmh041027/fndata.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载和数据预处理\n",
    "\n",
    "新建 process_dataset 函数用于数据加载和数据预处理，具体内容可见下面代码注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.dataset as ds\n",
    "from mindspore.dataset.transforms import TypeCast\n",
    "from mindspore.dataset.transforms import c_transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个预处理过程通过加载数据集、使用指定的分词器对文本进行分词和填充（使其长度一致），并将标签和分词结果转换为MindSpore所需的格式。它根据设备类型（Ascend或其他）选择适当的批处理方式，以生成适合模型训练的数据集。最终返回预处理后的数据集，以便于后续的模型训练和评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_dataset(source, tokenizer, max_seq_len=128, batch_size=32, shuffle=True):\n",
    "    is_ascend = mindspore.context.get_context('device_target') == 'Ascend'\n",
    "\n",
    "    column_names = [\"label\", \"text_a\"]\n",
    "    dataset = ds.GeneratorDataset(source, column_names=column_names, shuffle=shuffle)\n",
    "\n",
    "    def tokenize_and_pad(text):\n",
    "        tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=max_seq_len)\n",
    "        input_ids = np.asarray(tokenized['input_ids'], dtype=np.int32)\n",
    "        attention_mask = np.asarray(tokenized['attention_mask'], dtype=np.int32)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "    type_cast_op = TypeCast(mindspore.int32)\n",
    "\n",
    "    dataset = dataset.map(operations=tokenize_and_pad, input_columns=[\"text_a\"], output_columns=[\"input_ids\", \"attention_mask\"])\n",
    "    dataset = dataset.map(operations=type_cast_op, input_columns=[\"label\"], output_columns=\"labels\")\n",
    "\n",
    "    if is_ascend:\n",
    "        dataset = dataset.batch(batch_size)\n",
    "    else:\n",
    "        dataset = dataset.padded_batch(batch_size, pad_info={'input_ids': ([None], tokenizer.pad_token_id),\n",
    "                                                             'attention_mask': ([None], 0)})\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "昇腾NPU环境下暂不支持动态Shape，数据预处理部分采用静态Shape处理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将文本数据转换为BERT模型所需的输入格式，包括将文本分割成词片段（tokens），并将这些词片段映射到对应的词汇表索引，同时生成注意力掩码（attention masks）等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp.transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FALSE', 'Says the Annies List political group supports third-trimester abortions on demand.']\n",
      "['TRUE', 'Hillary Clinton agrees with John McCain \"by voting to give George Bush the benefit of the doubt on Iran.\"']\n",
      "['FALSE', 'Health care reform legislation is likely to mandate free sex change surgeries.']\n",
      "['TRUE', 'The Chicago Bears have had more starting quarterbacks in the last 10 years than the total number of tenured (UW) faculty fired during the last two decades.']\n",
      "['FALSE', 'Jim Dunnam has not lived in the district he represents for years now.']\n",
      "['TRUE', 'Says GOP primary opponents Glenn Grothman and Joe Leibham cast a compromise vote that cost $788 million in higher electricity costs.']\n",
      "['TRUE', '\"For the first time in history, the share of the national popular vote margin is smaller than the Latino vote margin.\"']\n",
      "['FALSE', '\"When Mitt Romney was governor of Massachusetts, we didnt just slow the rate of growth of our government, we actually cut it.\"']\n",
      "['TRUE', 'The economy bled $24 billion due to the government shutdown.']\n",
      "['FALSE', 'Most of the (Affordable Care Act) has already in some sense been waived or otherwise suspended.']\n"
     ]
    }
   ],
   "source": [
    "#展示数据集前十条\n",
    "with open(\"fndata/train.tsv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = f.read()\n",
    "lines = dataset.split(\"\\n\")\n",
    "s=0\n",
    "for line in lines[1:-1]:\n",
    "    print(line.split('\\t')[0:2])\n",
    "    s+=1\n",
    "    if s==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = process_dataset(SentimentDataset(\"fndata/train.tsv\"), tokenizer)\n",
    "dataset_val = process_dataset(SentimentDataset(\"fndata/valid.tsv\"), tokenizer)\n",
    "dataset_test = process_dataset(SentimentDataset(\"fndata/test.tsv\"), tokenizer, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'labels']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.get_col_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入ID张量：第一个张量，形状为 [32, 128]，每行代表一个句子的分词ID。BERT模型输入的每个句子长度被填充或截断为128（max_seq_len=128），101是BERT的CLS标记，用于表示句子的开始，后续是分词后的词汇表索引，0表示填充值。\n",
    "\n",
    "注意力掩码张量：第二个张量，形状同样为 [32, 128]，每个位置上1表示实际的词片段，0表示填充的部分。这个掩码用于在模型中忽略填充值对计算的影响。\n",
    "\n",
    "标签张量：第三个张量，形状为 [32]，每个值是一个标签，1表示真实消息，0表示虚假消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[32, 128], dtype=Int32, value=\n",
      "[[ 101, 1000, 2758 ...    0,    0,    0],\n",
      " [ 101, 1037, 2883 ...    0,    0,    0],\n",
      " [ 101, 2416, 4896 ...    0,    0,    0],\n",
      " ...\n",
      " [ 101, 2758, 2028 ...    0,    0,    0],\n",
      " [ 101, 1000, 2758 ...    0,    0,    0],\n",
      " [ 101, 1031, 1056 ...    0,    0,    0]]), Tensor(shape=[32, 128], dtype=Int32, value=\n",
      "[[1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " ...\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0],\n",
      " [1, 1, 1 ... 0, 0, 0]]), Tensor(shape=[32], dtype=Int32, value= [0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, \n",
      " 0, 1, 0, 0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "print(next(dataset_train.create_tuple_iterator()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建\n",
    "\n",
    "通过 BertForSequenceClassification 构建用于虚假消息检测的 BERT 模型，加载预训练权重，设置真假二分类的超参数自动构建模型。后面对模型采用自动混合精度操作，提高训练的速度，然后实例化优化器，紧接着实例化评价指标，设置模型训练的权重保存策略，最后就是构建训练器，模型开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "The following parameters in models are missing parameter:\n",
      "['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "from mindnlp.transformers import BertForSequenceClassification, BertModel\n",
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "# set bert config and define parameters for training\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "optimizer = nn.Adam(model.trainable_params(), learning_rate=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = Accuracy()\n",
    "# define callbacks to save checkpoints\n",
    "ckpoint_cb = CheckpointCallback(save_path='checkpoint', ckpt_name='bert_emotect', epochs=1, keep_checkpoint_max=2)\n",
    "best_model_cb = BestModelCallback(save_path='checkpoint', ckpt_name='bert_emotect_best', auto_load=True)\n",
    "\n",
    "trainer = Trainer(network=model, train_dataset=dataset_train,\n",
    "                  eval_dataset=dataset_val, metrics=metric,\n",
    "                  epochs=5, optimizer=optimizer, callbacks=[ckpoint_cb, best_model_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train will start from the checkpoint saved in 'checkpoint'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 255/255 [02:18<00:00,  1.84it/s, loss=0.6533276] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'bert_emotect_epoch_0.ckpt' has been saved in epoch: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 33/33 [00:06<00:00,  4.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.6583011583011583}\n",
      "---------------Best Model: 'bert_emotect_best.ckpt' has been saved in epoch: 0.---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 255/255 [02:15<00:00,  1.88it/s, loss=0.59775037]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: 'bert_emotect_epoch_1.ckpt' has been saved in epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 33/33 [00:06<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.6698841698841699}\n",
      "---------------Best Model: 'bert_emotect_best.ckpt' has been saved in epoch: 1.---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 255/255 [02:15<00:00,  1.88it/s, loss=0.48619908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of stored checkpoints has been reached.\n",
      "Checkpoint: 'bert_emotect_epoch_2.ckpt' has been saved in epoch: 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 33/33 [00:06<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.6496138996138996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 255/255 [02:15<00:00,  1.88it/s, loss=0.31588903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of stored checkpoints has been reached.\n",
      "Checkpoint: 'bert_emotect_epoch_3.ckpt' has been saved in epoch: 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 33/33 [00:06<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.6544401544401545}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 255/255 [02:15<00:00,  1.88it/s, loss=0.17769843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number of stored checkpoints has been reached.\n",
      "Checkpoint: 'bert_emotect_epoch_4.ckpt' has been saved in epoch: 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 33/33 [00:06<00:00,  4.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.6573359073359073}\n",
      "Loading best model from 'checkpoint' with '['Accuracy']': [0.6698841698841699]...\n",
      "---------------The model is already load the best model from 'bert_emotect_best.ckpt'.---------------\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.run(tgt_columns=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证\n",
    "\n",
    "将验证数据集加再进训练好的模型，对数据集进行验证，查看模型在验证数据上面的效果，此处的评价指标为准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluate: 100%|██████████| 32/32 [00:06<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate Score: {'Accuracy': 0.6545275590551181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(network=model, eval_dataset=dataset_test, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理\n",
    "\n",
    "遍历测试数据集，展示预测正确的条数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先加载测试数据集 dataset_infer，然后定义了一个用于预测文本真假的 predict 函数。在预测函数中，文本被分词器 tokenizer 分词并转换为张量格式 text_tokenized，然后通过模型 model 进行预测，获取预测的标签。根据预测结果和实际标签，更新统计变量 tru_num 和 fal_num，分别记录预测正确和错误的数量。最后，通过遍历 dataset_infer 中的每个文本和标签，对整个数据集进行预测，并打印数据集的总数、预测正确和错误的数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据总数是1016, 预测正确的有665条，预测错误的有351条\n"
     ]
    }
   ],
   "source": [
    "dataset_infer = SentimentDataset(\"fndata/test.tsv\")\n",
    "tru_num=0\n",
    "fal_num=0\n",
    "\n",
    "def predict(text, label):\n",
    "    label_map = {0: \"FALSE\", 1: \"TRUE\"}\n",
    "    global tru_num\n",
    "    global fal_num\n",
    "    text_tokenized = Tensor([tokenizer(text).input_ids])\n",
    "    logits = model(text_tokenized)\n",
    "    predict_label = logits[0].asnumpy().argmax()\n",
    "    info = f\"inputs: '{text}', predict: '{label_map[predict_label]}'\"\n",
    "    if label == predict_label:\n",
    "        tru_num+=1\n",
    "    else:\n",
    "        fal_num+=1\n",
    "\n",
    "from mindspore import Tensor\n",
    "s=0\n",
    "for label, text in dataset_infer:\n",
    "    predict(text, label)\n",
    "    s+=1\n",
    "\n",
    "print(f\"数据总数是{s}, 预测正确的有{tru_num}条，预测错误的有{fal_num}条\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "安装Gradio库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://repo.myhuaweicloud.com/repository/pypi/simple\n",
      "Collecting gradio\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/f1/1f/95a1bc5ec7cf8cdf92f2abf9e0bd59cbddf7870ae233aa33a99262cff9e1/gradio-4.37.2-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c5/19/5af6804c4cc0fed83f47bff6e413a98a36618e7d40185cd36e69737f3b0e/aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/46/30/2118537233fa72c1d91a81f5908a7e843a6601ccc68b76838ebc4951505f/altair-5.3.0-py3-none-any.whl (857 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.8/857.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/e6/33/de41e554e5a187d583906e10d53bfae5fd6c07e98cbf4fe5262bd37e739a/fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/1d/70/07914754979f5dd80bda947a0ffd181c08bfcb137b01c3c0cef45254d271/ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==1.0.2 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/53/4b/c527f41d89c609f2ca8697f6097feb6f9dbcb7d56c31cfabe24bf3f1c566/gradio_client-1.0.2-py3-none-any.whl (318 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.24.1 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/41/7b/ddacf6dcebb42466abd03f368782142baa82e08fc0c1f8eaa05b4bae87d5/httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (0.23.4)\n",
      "Collecting importlib-resources<7.0,>=1.3 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/75/06/4df55e1b7b112d183f65db9503bff189e97179b256e1ea450a3c365241e0/importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Collecting jinja2<4.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting markupsafe~=2.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5f/5a/360da85076688755ea0cceb92472923086993e86b5613bbae9fbc14136b0/MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting matplotlib~=3.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/8e/67/e75134cb83d2e533e46d72e2033a413772efdc18291beb981f5d574a829f/matplotlib-3.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0,>=1.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (1.26.4)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/d5/b5/ea16b9a62cd4f7a73077e9926cfb35e1e36d0a4aa34d235856142b1ff4a2/orjson-3.10.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (10.4.0)\n",
      "Collecting pydantic>=2.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/1f/fa/b7f815b8c9ad021c07f88875b601222ef5e70619391ade4a49234d12d278/pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydub (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a6/53/d78dc063216e62fc55f6b2eebb447f6a4b0a59f55c8406376f76bf959b08/pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/3d/47/444768600d9e0ebc82f8e347775d24aef8f6348cf00e9fa0e81910814e6d/python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/af/79/8a57016a761d11491b913460a3d1545cdbe96dca6acb1279102814c9147b/ruff-0.5.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/6a/23/8146aad7d88f4fcb3a6218f41a60f6c2d4e3a72de72da1825dc7c8f7877c/semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/68/4f/12207897848a653d03ebbf6775a29d949408ded5f99b2d87198bc5c93508/tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/20/b5/11cf2e34fbb11b937e006286ab5b8cfd334fde1c8fa4dd7f491226931180/typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio) (2.2.2)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b2/f9/e6f30ba6094733e4f9794fd098ca0543a19b07ac1fa3075d595bf0f1fb60/uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from gradio-client==1.0.2->gradio) (2024.5.0)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a6/9c/2356ecb952fd3992b73f7a897d65e57d784a69b94bb8d8fd5f97531e5c02/websockets-11.0.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.7/129.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonschema>=3.0 (from altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c8/2f/324fab4be6fe37fb7b521546e8a557e6cf08c1c1b3d0b4839a00f589d9ef/jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toolz (from altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b7/8a/d82202c9f89eab30f9fc05380daae87d617e2ad11571ab23d7c13a29bb54/toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting anyio (from httpx>=0.24.1->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/7b/a2/10639a79341f6c019dedc95bd48a4928eed9f1d1197f4c04f546fc7ae0ff/anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx>=0.24.1->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/78/d4/e5d7e4f2174f8a4d63c8897d79eb8fe2503f7ecc03282fee1fa2719c2704/httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
      "Collecting sniffio (from httpx>=0.24.1->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/e9/44/75a9c9421471a6c4805dbf2356f7c181a29c1879239abab1ea2cc8f38b40/sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/95/04/ff642e65ad6b90db43e668d70ffb6736436c7ce41fcc549f4e9472234127/h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from huggingface-hub>=0.19.3->gradio) (3.15.4)\n",
      "Requirement already satisfied: requests in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.19.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/31/a2/2f12e3a6e45935ff694654b710961b03310b0e1ec997ee9f416d3c873f87/contourpy-1.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.0/305.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib~=3.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib~=3.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c1/cb/b1877d606dfa1daca70324bf37afec2b0a386138c467580027b9b51188a8/fonttools-4.53.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/c0/a8/841594f11d0b88d8aeb26991bc4dac38baa909dc58d0c4262a4f7893bcbf/kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1 (from matplotlib~=3.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/9d/ea/6d76df31432a0e6fdf81681a895f009a4bb47b3c39036db3e1b528191d52/pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/78/b6/6307fbef88d9b5ee7421e68d78a9f162e0da4900bc5f5793f6d3d0e34fb8/annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/9b/f1/a006955715be98093d092aa025f604c7c00721e83fe04bf467c49f31a685/pydantic_core-2.20.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting click>=8.0.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/87/67/a37f6214d0e9fe57f6ae54b2956d550ca8365857f42a1ce0392bb21d9410/rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.7/240.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/fd/18/31fa32ed6c68ba66220204ef0be798c349d0a20c1901f9d4a794e08c76d8/starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/a1/03/89bf615052aa5453c04d952225ded0b88aab6487b9c5f0c268939d13b860/fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/5d/dd/b9a6027ba782b0072bf24a70929e15a58686668c32a37aebfcfaa9e00bdd/ujson-5.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/d7/ee/bf0adb559ad3c786f12bcbc9296b3f5675f529199bef03e2df281fa1fadb/email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
      "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/87/a1/8c5287991ddb8d3e4662f71356d9656d91ab3a36618c3dd11b280df0d255/dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ee/07/44bd408781594c4d0a027666ef27fab1e441b109dc3b76b4f836f8fd04fe/jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b7/59/2056f61236782a2c86b33906c025d4f4a0b17be0161b63b70fd9e8775d36/referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/97/b1/12238bd8cdf3cef71e85188af133399bfde1bddf319007361cc869d6f6a7/rpds_py-0.18.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/7c/58/d3728a369eaacd125918469c767e4af00326255db29e5e070433d9f40165/httptools-0.6.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/6a/3e/b68c118422ec867fa7ab88444e1274aa40681c606d59ac27de5a5588f082/python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/04/58/4d12d24220f2bf2c3125c74431035ddd7a461f9732a01cd5bd12f177370e/uvloop-0.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/ce/e8/607450608a0fc59bbe15448311ede44500a826ca467b5974a862d34e0290/watchfiles-0.22.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/ma-user/anaconda3/envs/python-3.9.0/lib/python3.9/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading http://repo.myhuaweicloud.com/repository/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=c2c20d8f92814f4c6eee2353da83b37873dc33ae55e51399ab6a9ce4d61596f8\n",
      "  Stored in directory: /home/ma-user/.cache/pip/wheels/52/69/46/472e20688b60b41afbd3840f0ae9f493a2123c806a784f8ce1\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, uvloop, ujson, toolz, tomlkit, sniffio, shellingham, semantic-version, ruff, rpds-py, python-multipart, python-dotenv, pyparsing, pydantic-core, orjson, mdurl, markupsafe, kiwisolver, importlib-resources, httptools, h11, fonttools, dnspython, cycler, contourpy, click, annotated-types, aiofiles, uvicorn, referencing, pydantic, matplotlib, markdown-it-py, jinja2, httpcore, email_validator, anyio, watchfiles, starlette, rich, jsonschema-specifications, httpx, typer, jsonschema, gradio-client, fastapi-cli, altair, fastapi, gradio\n",
      "Successfully installed aiofiles-23.2.1 altair-5.3.0 annotated-types-0.7.0 anyio-4.4.0 click-8.1.7 contourpy-1.2.1 cycler-0.12.1 dnspython-2.6.1 email_validator-2.2.0 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 fonttools-4.53.0 gradio-4.37.2 gradio-client-1.0.2 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 importlib-resources-6.4.0 jinja2-3.1.4 jsonschema-4.22.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 markdown-it-py-3.0.0 markupsafe-2.1.5 matplotlib-3.9.1 mdurl-0.1.2 orjson-3.10.6 pydantic-2.8.2 pydantic-core-2.20.1 pydub-0.25.1 pyparsing-3.1.2 python-dotenv-1.0.1 python-multipart-0.0.9 referencing-0.35.1 rich-13.7.1 rpds-py-0.18.1 ruff-0.5.0 semantic-version-2.10.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.37.2 tomlkit-0.12.0 toolz-0.12.1 typer-0.12.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Gradio integration here\n",
    "import gradio as gr\n",
    "from mindnlp.transformers import BertTokenizer, BertForSequenceClassification\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following parameters in checkpoint files are not loaded:\n",
      "['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "The following parameters in models are missing parameter:\n",
      "['classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "# # 加载预训练的BERT模型和Tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradio_predict接受输入文本text和对话历史history，然后使用预先训练的模型对文本进行预测。首先，它将文本进行标记化，确保输入数据是Tensor格式。接着，模型进行预测并输出logits，选择概率最高的标签作为预测结果，并将其映射为\"TRUE\"或\"FALSE\"。预测结果添加到对话历史中，并返回更新后的历史记录。如果过程中发生错误，函数会捕获异常并返回错误信息和原始历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradio_predict(text, history):\n",
    "    try:\n",
    "        label_map = {0: \"FALSE\", 1: \"TRUE\"}\n",
    "        tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='ms')\n",
    "        input_ids = tokenized['input_ids']\n",
    "        attention_mask = tokenized['attention_mask']\n",
    "\n",
    "        # 确保输入是正确的Tensor格式\n",
    "        if not isinstance(input_ids, Tensor):\n",
    "            input_ids = Tensor(input_ids)\n",
    "        if not isinstance(attention_mask, Tensor):\n",
    "            attention_mask = Tensor(attention_mask)\n",
    "\n",
    "        # 模型预测并提取logits\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predict_label = logits.asnumpy().argmax(axis=1)[0]     #获取最大值索引\n",
    "        prediction = label_map[predict_label]\n",
    "\n",
    "        # 添加到历史记录\n",
    "        history.append((text, prediction))\n",
    "        \n",
    "        return history, history\n",
    "    except Exception as e:\n",
    "        return f\"错误: {str(e)}\", history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义重置输入框函数\n",
    "def reset_user_input():\n",
    "    return gr.update(value='')\n",
    "\n",
    "# 定义重置状态函数\n",
    "def reset_state():\n",
    "    return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7870\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7870/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个 Gradio 界面 Blocks\n",
    "with gr.Blocks() as demo:\n",
    "    # 添加 HTML 标题\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">BERT 虚假消息检测</h1>\"\"\")\n",
    "\n",
    "    # 创建聊天机器人界面\n",
    "    chatbot = gr.Chatbot()\n",
    "    # 创建一行布局\n",
    "    with gr.Row():\n",
    "        # 创建一列布局，并设置比例\n",
    "        with gr.Column(scale=4):\n",
    "            # 在列内创建另一列布局，并设置比例\n",
    "            with gr.Column(scale=12):\n",
    "                # 添加用户输入文本框\n",
    "                user_input = gr.Textbox(show_label=False, placeholder=\"Input...\", lines=3, container=False)\n",
    "            # 创建另一列布局，用于按钮\n",
    "            with gr.Column(min_width=32, scale=1):\n",
    "                with gr.Row():\n",
    "                    # 添加推理按钮\n",
    "                    submitBtn = gr.Button(\"推理\", variant=\"primary\")\n",
    "                    # 添加清除历史按钮\n",
    "                    emptyBtn = gr.Button(\"清除历史\")\n",
    "\n",
    "    # 初始化历史记录状态\n",
    "    history = gr.State([])\n",
    "\n",
    "    # 绑定推理按钮点击事件，调用 gradio_predict 函数\n",
    "    submitBtn.click(gradio_predict, [user_input, history], [chatbot, history], show_progress=True)\n",
    "    # 绑定推理按钮点击事件，调用 reset_user_input 函数\n",
    "    submitBtn.click(reset_user_input, [], [user_input])\n",
    "\n",
    "    # 绑定清除历史按钮点击事件，调用 reset_state 函数\n",
    "    emptyBtn.click(reset_state, outputs=[chatbot, history], show_progress=True)\n",
    "\n",
    "# 启动 Gradio 界面，允许分享\n",
    "demo.queue().launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "AIGalleryInfo": {
   "item_id": "5094fc32-1c7c-4891-824a-a8b50efcc1fc"
  },
  "flavorInfo": {
   "architecture": "X86_64",
   "category": "CPU"
  },
  "imageInfo": {
   "id": "e1a07296-22a8-4f05-8bc8-e936c8e54202",
   "name": "mindspore1.7.0-cuda10.1-py3.7-ubuntu18.04"
  },
  "kernelspec": {
   "display_name": "python-3.9.0",
   "language": "python",
   "name": "python-3.9.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
